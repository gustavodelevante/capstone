import numpy as np
import tensorflow as tf
from tensorflow import keras
import os
import time
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, f1_score

# Set environment variable to reduce TensorFlow log verbosity
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Define a custom callback to stop training once accuracy reaches 94%
class AccuracyThresholdCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if logs is not None:
            if logs.get('val_accuracy') >= 0.94:  # Stop if validation accuracy >= 94%
                print("\nReached 94% accuracy so cancelling training!")
                self.model.stop_training = True

# Load images and handle errors
def load_dataset_with_error_handling(path, batch_size=32, seed=123, validation_split=0.2, subset='both'):
    # Reduced batch size to avoid OOM issues
    train_ds, test_ds = keras.utils.image_dataset_from_directory(
        path,
        image_size=(224, 224),
        batch_size=batch_size,  # Reduced batch size
        seed=seed,
        validation_split=validation_split,
        subset=subset
    )
    
    corrupted_images = []
    for images, labels in train_ds.concatenate(test_ds):
        for image, image_path in zip(images, train_ds.file_paths + test_ds.file_paths):
            if image is None:
                corrupted_images.append(image_path)
    
    if corrupted_images:
        print("Corrupted images:")
        for corrupted_image in corrupted_images:
            print(corrupted_image)

    return train_ds, test_ds

# Enable GPU usage and restrict memory growth to avoid OOM errors
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print("GPUs detected: ", len(gpus))
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)  # Limit GPU memory growth
else:
    print("No GPUs detected. Training will use CPU.")

path = r"C:\Users\gustavo\Desktop\Capstone\Crops Data\Cashew_only_inval"
train_ds, test_ds = load_dataset_with_error_handling(path)

# Define the neural network model
model = keras.Sequential([
    keras.layers.Rescaling(scale=1/255, input_shape=(224, 224, 3)),
    tf.keras.Sequential([
        tf.keras.layers.RandomFlip(),
        tf.keras.layers.RandomRotation(0.2),
    ], name='augmentation'),
    keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D((2, 2)),
    keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D((2, 2)),
    tf.keras.layers.Dropout(0.2),
    keras.layers.Conv2D(256, (3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D((2, 2)),
    tf.keras.layers.Dropout(0.4),
    keras.layers.Conv2D(256, (3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D((2, 2)),
    tf.keras.layers.Dropout(0.5),
    keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D((2, 2)),
    keras.layers.Dropout(0.6),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(21, activation='softmax')  # 'softmax' for multi-class classification
])

# Compile the model using Adam optimizer
model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

checkpoint_filepath = r"C:\Users\gustavo\Desktop\Capstone\Model"
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    save_freq='epoch'
)

accuracy_threshold_callback = AccuracyThresholdCallback()

# Define EarlyStopping to halt training if accuracy decreases
early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    patience=30,  # Early stopping if no improvement after 10 epochs
    mode='auto',
    restore_best_weights=True
)

# Measure training time
start_time = time.time()

# Train the model with the callbacks
history = model.fit(
    train_ds,
    validation_data=test_ds,
    epochs=250,
    callbacks=[checkpoint_callback, accuracy_threshold_callback, early_stopping_callback]
)

end_time = time.time()
training_duration = end_time - start_time

print(f"Training completed in {training_duration:.2f} seconds")

# Plot training history
def plot_history(history):
    plt.figure(figsize=(12, 6))

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Accuracy by Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss by Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Plot the training history
plot_history(history)

# Evaluate model and print precision and F1 score
def evaluate_model(model, test_ds):
    y_true = np.concatenate([y.numpy() for x, y in test_ds], axis=0)
    y_pred = model.predict(test_ds)
    y_pred = np.argmax(y_pred, axis=1)

    precision = precision_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    print(f"Precision: {precision:.4f}")
    print(f"F1 Score: {f1:.4f}")

evaluate_model(model, test_ds)
